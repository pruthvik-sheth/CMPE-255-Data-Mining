{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Heart Attack Risk Prediction using Gradient Boosting\n",
        "In this notebook, we will predict the risk of heart attacks based on various medical, lifestyle, and demographic factors. We will apply the Gradient Boosting Classifier, a powerful machine learning algorithm for classification tasks.\n",
        "\n",
        "### Steps involved:\n",
        "1. Load the dataset\n",
        "2. Data Cleaning and Preprocessing\n",
        "3. Split the data into training and testing sets\n",
        "4. Standardize the data\n",
        "5. Train a Gradient Boosting Classifier\n",
        "6. Evaluate the model\n",
        "7. Perform a sample prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load the Dataset\n",
        "We begin by loading the heart attack dataset, which contains various features related to patients' health, lifestyle, and demographic factors.\n",
        "The dataset includes information such as age, cholesterol levels, blood pressure, exercise hours, and more.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('/content/heart_attack_prediction_dataset.csv')\n",
        "# Preview the first few rows of the dataset\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Data Cleaning and Preprocessing\n",
        "Now we will clean and preprocess the data to make it suitable for machine learning.\n",
        "\n",
        "### Key preprocessing steps:\n",
        "- Split blood pressure into two features: systolic and diastolic pressure\n",
        "- Convert categorical variables (e.g., sex, diet) to numerical values\n",
        "- Apply one-hot encoding for country, continent, and hemisphere features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data cleaning and processing\n",
        "data[['Systolic BP', 'Diastolic BP']] = data['Blood Pressure'].str.split('/', expand=True)\n",
        "data['Systolic BP'] = pd.to_numeric(data['Systolic BP'], errors='coerce')\n",
        "data['Diastolic BP'] = pd.to_numeric(data['Diastolic BP'], errors='coerce')\n",
        "data['Sex'] = data['Sex'].apply(lambda x: 1 if x == 'Male' else 0)\n",
        "data['Diet'] = data['Diet'].apply(lambda x: 1 if x == 'Healthy' else 0)\n",
        "data = pd.get_dummies(data, columns=['Country', 'Continent', 'Hemisphere'], drop_first=True)\n",
        "data_cleaned = data.drop(columns=['Blood Pressure', 'Patient ID'])\n",
        "# Display cleaned data\n",
        "data_cleaned.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Splitting the Data\n",
        "We will now split the data into training and testing sets.\n",
        "The training set will be used to build the model, while the testing set will be used to evaluate how well the model performs on unseen data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define features (X) and target variable (y)\n",
        "X = data_cleaned.drop('Heart Attack Risk', axis=1)\n",
        "y = data_cleaned['Heart Attack Risk']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Standardizing the Data\n",
        "Standardization is an important step in machine learning, especially for algorithms like Gradient Boosting.\n",
        "Standardizing ensures that all numerical features have the same scale, which helps the model learn more efficiently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standardize the numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Train the Gradient Boosting Classifier\n",
        "We will now train a Gradient Boosting Classifier on the training data.\n",
        "This model builds multiple decision trees and combines them to make accurate predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and train the Gradient Boosting model\n",
        "gb_model = GradientBoostingClassifier(random_state=42)\n",
        "gb_model.fit(X_train_scaled, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Evaluate the Model\n",
        "We will now evaluate the trained model using the testing data.\n",
        "We will calculate key metrics such as precision, recall, and F1-score to assess how well the model performs in predicting heart attack risk.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions and evaluate the model\n",
        "y_pred = gb_model.predict(X_test_scaled)\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Perform a Sample Prediction\n",
        "Finally, we will use the trained model to make a prediction for a sample patient.\n",
        "We will provide the model with the features of a test patient and check the predicted risk for a heart attack.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform a sample prediction using the first test example\n",
        "sample_input = [X_test_scaled[0]]\n",
        "sample_prediction = gb_model.predict(sample_input)\n",
        "print(f'Predicted Heart Attack Risk (0 = No Risk, 1 = High Risk): {sample_prediction[0]}')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
